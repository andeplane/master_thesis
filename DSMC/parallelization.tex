\section{Parallelization}
\label{sec:dmsc_parallelization}
% Keywords: MPI, spatial domain geometry, communication surface, 6 facets, world geometry, 
Each collision cell is completely independent of each other. We can then divide the spatial domain into subdomains, each fully controlled by one processor. Each processor is responsible for executing the timestep for every particle in the corresponding volume. The processors will usually contain many collision cells as illustrated in figure \ref{fig:dsmc_parallelization_1}. We will use the terms \textit{processor}, \textit{node} and \textit{CPU} interchangeably.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\textwidth, trim=0cm 0cm 0cm 0cm, clip]{DSMC/figures/parallelization.eps}
\end{center}
\caption{Illustration of how the spatial domain can be divided into four subdomains, each controlled by a processor. Each processor contains many particles that are placed in several collision cells (marked grey).}
\label{fig:dsmc_parallelization_1}
\end{figure}
If we first assume that all the processors have full knowledge about the geometry of the full system, the only thing we have to take care of is when particles move from one processor to another. We have used (MPI) for the communcation between processors, and it is assumed that the reader is familiar with how MPI works. 
\subsection{Topological structure}
The processors are divided into a three dimensional grid with $(P_x, P_y, P_z)$ being the number of CPU's in each dimension, yielding a total of $P = P_x\cdot P_y\cdot P_z$ processors. We can then use the grid coordinates $(p_x, p_y, p_z)$ to uniquely label the processors as shown in figure \ref{fig:dsmc_parallelization_2}.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\textwidth, trim=0cm 0cm 0cm 0cm, clip]{DSMC/figures/parallelization_node_configuration.eps}
\end{center}
\caption{Processor labeling in a 3-dimensional grid. Each processor is uniquely identified through its coordinate $(p_x, p_y, p_z)$.}
\label{fig:dsmc_parallelization_2}
\end{figure}
When starting a program with MPI, each process is provided a unique identification number $p$ in the range $[0, P-1]$ for $P$ processors. This can be mapped to the 3-dimensional grid coordinates through
\begin{align}
	\nonumber
	p_x(p) &= {p \over P_yP_z}\\
	\nonumber
	p_y(p) &= {p \over P_z} \bmod P_y\\
	p_z(p) &= p \bmod P_z
\end{align}
whereas the inverted mapping is 
\begin{align}
	p(p_x, p_y, p_z) = p_x\cdot P_zP_y + p_y\cdot P_z + p_z.
\end{align}
With the processor id $p$ given, it is easy to determine which subvolume this processor should control. If the system is of size $L^{(i)}$ in the $i$'th dimension, we can find the \textit{node length} $L_{\text{node}}^{(i)} \equiv l_i = L_i/P_i$. A processor with coordinates $(p_x, p_y, p_z)$ will control all particles with coordinates in the range
\begin{align}
	\nonumber
	x&\in[p_xl_x, (p_x+1)l_x\rangle\\
	\nonumber
	y&\in[p_yl_y, (p_y+1)l_y\rangle\\
	z&\in[p_zl_z, (p_z+1)l_z\rangle.
\end{align}
Since the collision cells are independent of each other, collisions will happen in parallel where each processor loops through all of its cells colliding the particles as described in section \ref{sec:dsmc_collision_cells}. 
\subsection{Exchanging particles}
During a timestep, a particle can move from one processor to one of the 26 neighbouring nodes (the middle node in a $3\times3\times3$ grid has a 26 neighbours). After each timestep, all processors loop through their particles to find the ones having moved out of the processor's spatial domain. This process is illustrated in listing \ref{lst:dsmc_detecting_particles_moved_out}.
\begin{lstlisting}[caption=Detecting which particles having moved out of a processor's spatial domain., label=lst:dsmc_detecting_particles_moved_out]
double mpi_move() {
	for(int n=0; n<num_particles_local; n++) {
		int node_id = topology->index_from_particle_index(n);
		if(node_id != myid) {
			// Particle belongs to another node
		}
	}
}
\end{lstlisting}
In principle, there are 26 potential recieving nodes, so each node needs to be able to send information to all of them. The easiest way to implement this is to let each processor directly communicate with all of its neighbours. However, this approach requires a lot more communcation time than actually needed.\\
If we instead send information about these particles through a 3-step process, we can reduce the number of neighbouring nodes from 26 to 6. This idea is best illustrated in two dimensions, but is easily generalized to the three-dimensional case, see figure \ref{fig:parallelization_facet_technique}.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\textwidth, trim=0cm 0cm 0cm 0cm, clip]{DSMC/figures/parallelization_facet_technique.eps}
\end{center}
\caption{The middle node (1,1) has 8 neighbours it needs to communicate with. Each node only needs to communicate with its nearest neighbours (4 in two dimensions, 6 in three dimensions), because the nearest neighbours can work as intermediate information carriers. A particle that moves from processor (1,1) to (2,2) will in step 1 be sent to (2,1), then in step 2 be sent to (2,2).}
\label{fig:parallelization_facet_technique}
\end{figure}
An analysis of the parallelization is studied in subsection \ref{sec:dsmc_parallelization_performance} where we compare how well the program performance scales with increasing number of processors. 