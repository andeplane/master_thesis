\section{Future work}
No matter how much work we do, new ideas pop our minds faster than we manage to complete the old ones. During the past months, through everything we have learned, both ideas about extensions to the models and applications have emerged. First of all, as we discussed in section \ref{sec:dsmc_steady_state}, our analysis of whether or not the system has reached a steady state is inadequate. New, automatic methods should be incorporated by, in several regions (for example by using the collision cells in DSMC), looking at expectation values and the variance of physical quantities such as momentum, energy and temperature. Even with a more advanced analysis of if the steady state is reached or not, reaching a steady state may take a long time. In the benchmark we performed and discussed in section \ref{sec:dsmc_parallelization_performance}, 512 processors used 545 seconds to proceed 500 timesteps with one billion particles. If the system requires 100k timesteps to reach a steady state, this would require 15k cpu hours just to reach the steady state. With DSMC, it may be possible to use a smaller number of particles (that is, increase the number of effective atoms per simulated particle) while the system reaches a steady state. Then, before the sampling, increase the total number of particles by distributing a number of particles with the velocities and densities computed from the previous simulation, equilibrating this state before starting the sampling.

A detailed study of how the Cercignani-Lampis collision model from section \ref{sec:surface_interactions} affects the permeability would also be a interesting and important work. Here one could for example also use the Lennard-Jones MD model to fire single atoms towards a surface and gather a distribution to compare with the Cercignani-Lampis model.

With the voxelization model of the surface we developed in section \ref{sec:dsmc_complex_geometries} a few questions quickly arise. What are the effects of the discretization of the geometry? A real, smooth surface does indeed look different than a jagged surface composed by voxels. With this representation, the effective surface area is increased which could affect the flow. A detailed study of these effects should be carried out. Such a representation also requires a lot of memory as the memory requirement scales as $O(N^3)$. To save memory, we can use a sparse voxel octree representation. If a larger group of voxels all share the same value, only one, larger voxel is saved in memory, representing all the smaller voxels.

Since the matrix already is on a discretized grid and the net momentum transfer from the particles is calculated during collisions, it is possible to extend the model to include deformation and cracking which are important processes inside the shales. Production from organic material can also be included by including a diffusion solver on the matrix that can desorb and adsorb gas particles.

For a large number of processors, the amount of work each processor is assigned may not be equal to all the others. This means that a processor that is computing only half the work than another processor, it will spend 50$\%$ of its time waiting for the other processors to finish. The solution to this is known as load balancing. An analysis of the system prior to the simulation may distribute the amount of work not by dividing the system into equal \textit{total volumes}, but equal \textit{pore volumes} in which the particles are. The amount of work is proportional to the number of particles a processor has. A last, but very interesting task we would like to do is simulate gas through a nanoporous media from a real material. Such data can be obtained with a technique called focused ion beam scanning electron microscopy (FIB-SEM). 