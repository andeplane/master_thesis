The kinetic theory of gases is a microscopic theory that describes the behaviour of gases on the molecular level. A system of $N$ particles is fully described by the $3N$ momentum components combined with the $3N$ spatial coordinates. Together, this forms a $6N$ dimensional phase space where each point represents the state of the system in an ensemble. We start this chapter by introducing the distribution function, the concepts of microstates, macrostates and ensembles in section \ref{sec:kinetic_theory_distribution_function}, before we in section \ref{sec:kinetic_theory_ensemble_averages} explain how we measure the macroscopic observables which are average values over all the states in an ensemble. Then we have a brief discussion about ergodicity in section \ref{sec:kinetic_theory_ergodicity} which is a very important assumption when we start measuring physical quantities in a numerical statistical mechanics model such as Direct Simulation Monte Carlo (chapter \ref{chap:dsmc}) and Molecular Dynamics (chapter \ref{chap:md}). In section \ref{sec:boltzmann_equation} we derive the Boltzmann equation which is the fundamental equation that governs the behavior of the distribution function. We then define what an equilibrium state is which we use to derive the Maxwell-Boltzmann velocity distribution in section \ref{sec:maxwell_boltzmann_distribution}. As we might remember, the Knudsen number is an important dimensionless quantity that we use to quantify how important surface and non-continuum effects are. The Knudsen number is the ratio between the mean free path $\lambda$ and some characteristic length $L$ of the system. In section \ref{sec:mean_free_path_calculation} we calculate the mean free path which is used to compute the mean collision time $\tau_\text{coll}$, which is important to choose a good timestep in the Direct Simulation Monte Carlo model in chapter \ref{chap:dsmc}.
\section{The distribution function}
\label{sec:kinetic_theory_distribution_function}
A point $(\vec r, \vec v)$ in the phase space describes what is called a \textit{microstate} and contains a massive amount of information. Given this point, we would know the position and velocity to \textit{every} particle in the entire system. In a liter of an ideal gas under standard pressure, the number of particles is of order $10^{22}$ \cite{garcia2000numerical}, so if each of these $6N$ coordinates were represented as an 8 byte \textit{double} on a computer, we would need more than $10^{11}$ terabytes of memory just to store all the information. This approach would be very inconvenient and, fortunately, not at all necessary. The really interesting properties in a system are the macroscopic ones, like energy, temperature, pressure, volume, average velocity among others. For example, the total energy in a gas consisting of $N$ particles is calculated as
\begin{align*}
	E = \sum_{i=1}^N \frac{1}{2} m_i v_i^2 + V(\vec r),
\end{align*}
where $m_i$ is the mass of particle $i$, $v_i$ is its scalar velocity and $V(\vec r)$ is the total potential energy in the system depending on the full $3N$-dimensional spatial coordinate $\vec r$.\\
Given a microstate, what happens if we switch two particles, say particle $i$ and $j$? If particle $i$ had velocity $\vec v_i = \vec u$ and particle $j$ had some other velocity $\vec v_j = \vec w$, we could quickly swap them so that $\vec v_i = \vec w$ and $\vec v_j = \vec u$ (theoretically of course, it would be a difficult task in an experiment). If their masses are identical, the total energy of the system would not change, but since \textit{we} know that we switched the two particles, we could want to count this as another microstate. We could in principle paint the particles with different colors, or maybe just label them with their own unique number. However, in a real, monoatomic gas, we can't really tell the difference if particle $i$ and $j$ secretly agreed to switch places without telling us. If they did so, it would not count as different microstates, the system remains exactly the same. We say that the particles are \textit{indistinguishable}.\\
If we instead increase the velocity of particle $i$, we can reduce some another particle $j$'s velocity to keep the total energy constant. Even though the macroscopic property \textit{energy} is unchanged, there is a (theoretically) measureable difference between these two states. The set of all microstates that share the same macroscopic state variables (a \textit{macrostate}) forms an ensemble of systems. A much used ensemble is the microcanonical ensemble (NVE) with a constant number of particles $N$, constant volume $V$ and constant energy $E$. Increasing the velocity of particle $i$ while at the same time reducing particle $j$'s velocity just enough to remain the energy unchanged does not change the particle number $N$, the volume $V$ or the energy. So these two different microstates would both be in the same ensemble.\\

In a typical system, the number of microstates in a macrostate is so huge that the phase space points can be described by a continuous density function $f(\vec p, \vec r, t)$ without losing any important information \cite{mcquarrie1973statistical}. The input parameters are the $3N$ momentum components, the $3N$ spatial coordinates plus time. This function is often called a \textit{distribution function}, normalized so that
\begin{align}
	\iint\! f(\vec p, \vec r, t) \dm \vec p\dm \vec r = N,
\end{align}
where $\dm \vec p\dm \vec r=\dm p_1\dm p_2...\dm r_{3N}$ is the $6N$ dimensional phase space volume element. This density function does not contain the information about the \textit{exact} positions or momenta of the particles, but the \textit{probability} to find the system in a state around a given phase space point. We can then use it to calculate measurable, macroscopic average values. 

\section{Ensemble averages}
\label{sec:kinetic_theory_ensemble_averages}
Given the distribution function $f$, we can calculate any ensemble average (which will be the measurable, macroscopic properties of the system) by interpreting $f$ as a probability distribution (it needs the factor $1/N$ to be normalized to one) that gives the probability of finding a particle at position $\vec r + \dm \vec r$ with momentum in the range $\vec p + \dm\vec p$ at the time $t$. We can then use the standard expectation value expression to calculate a macroscopic property $\bar A$
\begin{align}
	\label{eq:ensemble_average}
	\bar A(t) = \frac{1}{ N}\iint\! A(\vec p, \vec r, t)f(\vec p, \vec r, t)\dm \vec p\dm\vec r.
\end{align}
This could for example be the total energy
\begin{align}
	\bar E(t) &= \frac{1}{ N}\iint\! E(\vec p, \vec r, t)f(\vec p, \vec r, t)\dm \vec p\dm\vec r \\
	&= \frac{1}{ N}\iint\! \left(V(\vec r) + \sum_{i=1}^N \frac{\vec p_i^2}{2m_i} \right)f(\vec p, \vec r, t)\dm \vec p \dm \vec r,
\end{align}
where $\vec p_i$ is the momentum of particle $i$. Any other quantity of interest can in principle be measured in the same way. 

\section{Ergodicity}
\label{sec:kinetic_theory_ergodicity}
The ensemble average calculates the average value of some macroscopic quantity given the distribution function $f$. Usually, we don't have the distribution function, except in some very simple theoretical calculations. Even then, it might be difficult to compute the integral in equation \eqref{eq:ensemble_average}. The usual situation when we do numerical statistical mechanics is that we have a way to explore the phase space, hoping that it helps us visit states with probabilities according to the given ensemble. Many Monte Carlo techniques (such as the Metropolis algorithm) allow us to go to a new, random point in the phase space, and calculate the probability of going from the current state to the new state. From this, we can count the number of times we have visited different regions of the phase space and create a histogram or maybe, if we're lucky, fit some existing probability distribution to our data.\\
Another approach is to let the rules of physics take us around in the phase space, from one state to another, following Newton's equations of motion. Imagine that at $t=0$, our system is in some microscopic state (a single phase space point ($\vec r(0), \vec p(0)$)) and at a later time $t=\tau$ has moved to ($\vec r(\tau), \vec p(\tau)$). Between these two points, the system has moved through many other points, exploring the phase space. It seems reasonable that in the limit of infinite time, the time evolution should visit the phase space points according to the density given by $f$. If not, it wouldn't make any sense even talking about $f$, since it is the time evolution we, as humans, are experiencing in the real physical world.\\
This is called the ergodicity hypothesis, the assumption that a system following the laws of physics, explores the phase space with the probability of being in a region proportional to the density in that region. The average value of a macroscopic quantity $A$ is then found as
\begin{align}
	\bar A = \lim_{t\rightarrow \infty} \frac{1}{t}\int_0^{t} A(\vec r(t'), \vec p(t'), t')\dm t'.
\end{align}